{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wikipedia_webscraping.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5Fzuld0xBzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import operator\n",
        "import requests\n",
        "import json\n",
        "from tabulate import tabulate\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR_xAOrFxFvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getWordsFromUrl(url):\n",
        "  \n",
        "  word_list = []\n",
        "  \n",
        "  webpage = requests.get(url)\n",
        "  \n",
        "  plain_text_webpage = webpage.text\n",
        "    \n",
        "  bs = BeautifulSoup(plain_text_webpage, 'lxml')\n",
        "    \n",
        "  for data in bs.findAll('p'):\n",
        "    if data.text is None:\n",
        "      continue\n",
        "    \n",
        "    text = data.text\n",
        "    \n",
        "    text = text.lower().split()\n",
        "    \n",
        "    for word in text:\n",
        "      cleaned_word = getCleanWord(word)\n",
        "      \n",
        "      if len(cleaned_word) > 0:\n",
        "        word_list.append(cleaned_word)\n",
        "  \n",
        "  return word_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mrggeh-ix1xp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getCleanWord(word):\n",
        "  return re.sub(r'\\W+', '', word) # [^a-zA-Z0-9]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwsJLHk02IXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getWordFrequency(words):\n",
        "  \n",
        "  word_dict = {}\n",
        "  \n",
        "  for word in words:\n",
        "    if word in word_dict:\n",
        "      word_dict[word] += 1\n",
        "    else:\n",
        "      word_dict[word] = 1\n",
        "      \n",
        "  return word_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPAoTVCf2NRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getSortedWords(words):\n",
        "  return sorted(words.items(), key=operator.itemgetter(1), reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmRHls874GED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def removeStopWords(words):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  \n",
        "  word_list = []\n",
        "  \n",
        "  for word, count in words:\n",
        "    if word not in stop_words:\n",
        "      word_list.append([word, count])\n",
        "  \n",
        "  return word_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA08gb7r5J_a",
        "colab_type": "code",
        "outputId": "11aaab93-3b4a-4071-ac92-1668e05f1604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R-DFuxU-JI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "string_query = \"ai\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAWL8uxjxwS0",
        "colab_type": "code",
        "outputId": "d08032dc-084a-48dc-834b-bd93651c311c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        }
      },
      "source": [
        "#access wiki API. json format. query it for data. search tyep. shows list of possibilities\n",
        "wikipedia_api_link = \"https://en.wikipedia.org/w/api.php?format=json&action=query&list=search&srsearch=\"\n",
        "wikipedia_link = \"https://en.wikipedia.org/wiki/\"\n",
        "\n",
        "url = wikipedia_api_link + string_query\n",
        "\n",
        "try:\n",
        "  response = requests.get(url)\n",
        "\n",
        "  data = json.loads(response.content.decode('utf-8'))\n",
        "\n",
        "  page_tag = data['query']['search'][0]['title']\n",
        "\n",
        "  url = wikipedia_link + page_tag.replace(' ', '_')\n",
        "\n",
        "  web_page_words = getWordsFromUrl(url)\n",
        "\n",
        "  word_count = getWordFrequency(web_page_words)\n",
        "\n",
        "  sorted_words = getSortedWords(word_count)\n",
        "\n",
        "  final_list = []\n",
        "\n",
        "  for word, count in removeStopWords(sorted_words)[:20]:\n",
        "    percentage_of_word = float((count * 100) / len(word_count))\n",
        "    final_list.append([word, count, percentage_of_word])\n",
        "    \n",
        "except requests.exceptions.Timeout:\n",
        "  print(\"The server didn't respond. Please, try again later.\")\n",
        "  \n",
        "print(\" +--------+\")\n",
        "print(\" > SUMMARY \")\n",
        "print(\" +--------+\")\n",
        "print(\"- Wikipedia Api Link: \", wikipedia_api_link)\n",
        "print(\"- Wikipedia Official site link: \", wikipedia_link)\n",
        "print(\"- Query: \", string_query)\n",
        "print(\"- Link: \", url)\n",
        "print(\"- Total words: \", len(web_page_words))\n",
        "print(\"- Total cleaned words: \", len(word_count))\n",
        "\n",
        "print(\"+-----------------------------------------------------+\")\n",
        "print(tabulate(final_list, headers=[\"Word\", \"Frequency\", \"Frequency Percentage\"], tablefmt='orgtbl'))\n",
        "print(\"+-----------------------------------------------------+\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " +--------+\n",
            " > SUMMARY \n",
            " +--------+\n",
            "- Wikipedia Api Link:  https://en.wikipedia.org/w/api.php?format=json&action=query&list=search&srsearch=\n",
            "- Wikipedia Official site link:  https://en.wikipedia.org/wiki/\n",
            "- Query:  ai\n",
            "- Link:  https://en.wikipedia.org/wiki/Artificial_intelligence\n",
            "- Total words:  12646\n",
            "- Total cleaned words:  3388\n",
            "+-----------------------------------------------------+\n",
            "| Word         |   Frequency |   Frequency Percentage |\n",
            "|--------------+-------------+------------------------|\n",
            "| ai           |         167 |               4.92916  |\n",
            "| intelligence |          77 |               2.27273  |\n",
            "| artificial   |          66 |               1.94805  |\n",
            "| human        |          62 |               1.82999  |\n",
            "| learning     |          51 |               1.50531  |\n",
            "| many         |          50 |               1.4758   |\n",
            "| machine      |          47 |               1.38725  |\n",
            "| research     |          40 |               1.18064  |\n",
            "| knowledge    |          36 |               1.06257  |\n",
            "| use          |          34 |               1.00354  |\n",
            "| neural       |          34 |               1.00354  |\n",
            "| networks     |          33 |               0.974026 |\n",
            "| would        |          32 |               0.94451  |\n",
            "| also         |          31 |               0.914994 |\n",
            "| computer     |          29 |               0.855962 |\n",
            "| used         |          29 |               0.855962 |\n",
            "| problems     |          29 |               0.855962 |\n",
            "| machines     |          27 |               0.79693  |\n",
            "| humans       |          27 |               0.79693  |\n",
            "| systems      |          26 |               0.767414 |\n",
            "+-----------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayoDs8_50u3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}